{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Regression Models in Pyhton</h1></center>\n",
    "<center><h3>2023-03-01</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Machine Learning vs. \"Classical\" Statistics\n",
    "\n",
    "1. Goals\n",
    "2. Data Requirements\n",
    "3. Interpretability\n",
    "4. Application Areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Goals\n",
    "\n",
    "* Statistical Inference: \n",
    "  - Make conclusions about a population based on a sample of data\n",
    "  - Use statistical methods to estimate parameters of interest, such as the population mean or proportion\n",
    "  - Test hypotheses about these parameters\n",
    "\n",
    "* Machine Learning: \n",
    "  - Develop models that can accurately predict outcomes on new, unseen data\n",
    "  - Training models on a labeled dataset, where the outcomes are known\n",
    "  - Use this training data to build a model that can generalize to new data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Requirements\n",
    "\n",
    "* Sample Size and Sampling Methods\n",
    "  - Statistical inference often requires a random sample of data from a population, and the size of the sample is an important consideration for the accuracy of the inference. Different sampling methods may also be used to obtain representative samples. \n",
    "  - Machine learning models can work with datasets of various sizes, and sampling methods may not be relevant in some cases\n",
    "\n",
    "* Data Structure\n",
    "  - Statistical inference often assumes that data is structured in a certain way (e.g., normally distributed), or that there is a linear relationship between variables. \n",
    "  - In contrast, machine learning can handle unstructured data, such as text, images, and audio, and can also handle noisy data that may contain missing or irrelevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Interpretability\n",
    "\n",
    "* Model complexity\n",
    "  - Classical statistics uses simpler models that are easier to interpret, such as linear regression or ANOVA\n",
    "  - ML often uses more complex models such as neural networks or decision trees, which can be more difficult to interpret, particularly when they involve many layers or interactions between variables.\n",
    "\n",
    "* Trade-off between accuracy and interpretability\n",
    "  - ML models face trade-off between model accuracy and interpretability; more complex models may achieve higher accuracy, but may be more difficult to interpret, while simpler models may be easier to understand but may sacrifice some accuracy. \n",
    "  - Classical statistics models may also face this trade-off to some extent, but may be designed to prioritize interpretability over accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Application Areas\n",
    "\n",
    "* Statistical Inference\n",
    "  - Commonly used in fields such as economics, social sciences, and healthcare\n",
    "  - Fields where researchers seek to draw inferences about populations from limited data samples\n",
    "* Machine Learning\n",
    "  - Widely used in areas such as image recognition, natural language processing, and recommendation systems\n",
    "  - Where the focus is on building accurate predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Linear Regression Review\n",
    "\n",
    "Linear regression modeling is a technique used to generate models for outcome variables that are continuous (e.g., heart rate, age, income, cholesterol, stock price, GDP, etc.)\n",
    "\n",
    "Our goal for linear regression in machine learning context is slightly different than in classical statistics. In particular, we are less interested in the \"significance\" of a given predictor variable, and we are more interested in the overall quality of the model in terms of it's performance on new data. \n",
    "\n",
    "We can use anywhere from 1 to _p_ predictor variables in our model for the outcome variable. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## $$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... \\beta_p x_{pi} + \\epsilon_i$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![image](images/linear_reg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=images/train_test_split.png width = 1080/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# SciKit-Learn\n",
    "\n",
    "* Popular open-source Python library for machine learning tasks, including:\n",
    "  - classification\n",
    "  - regression\n",
    "  - clustering\n",
    "\n",
    "* The library provides a wide range of tools for data preprocessing, feature extraction, model selection, and evaluation\n",
    "\n",
    "* Follows a consistent API, so it's easy to use and integrate with other Python libraries\n",
    "\n",
    "* Also includes extensive documentation and examples to help users get started with machine learning\n",
    "\n",
    "* The library is built on top of NumPy and SciPy, two other popular Python libraries for scientific computing, which means that it is fast and efficient, even for large datasets.\n",
    "\n",
    "* Widely used in industry and academia and is a valuable tool for anyone interested in machine learning and data science. It is also actively maintained and updated by a large community of contributors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Importing SciKit-Learn Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Diabetes Study \n",
    "\n",
    "* 442 diabetes patients were measured on 10 baseline variables. \n",
    "\n",
    "* Dependent variable is a measure of disease progression one year after baseline\n",
    "\n",
    "* Goal is to build a prediction model for the response variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "diabetes_df = pd.read_csv(\"data/hastie_diabetes.csv\")\n",
    "\n",
    "print(diabetes_df.shape)\n",
    "\n",
    "diabetes_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Train/Test Split \n",
    "\n",
    "* Need to separate our data in to training and test sets\n",
    "* Use random number generator\n",
    "  - Set the seed for reproducibility (not generally something we would do in practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = diabetes_df.iloc[:, :-1].values         # get X values (i.e., predictors/features)\n",
    "y = diabetes_df.loc[:, \"y\"].values          # get y values (i.e., outcome/target variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(137)\n",
    "\n",
    "# Take random draws from binomial to determine train/test split\n",
    "is_train = np.random.binomial(1, 0.8, size = diabetes_df.shape[0]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use is_train vector for boolean indexing\n",
    "X_train = X[is_train, ]\n",
    "X_test = X[is_train == False]\n",
    "y_train = y[is_train]\n",
    "y_test = y[is_train == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Fitting Linear Regression Model on Diabetes Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "mod = LinearRegression()       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)      # fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mod.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Measures of Model Fit\n",
    "\n",
    "* R-squared (R²): R-squared is a measure of how well the linear regression model fits the data. It ranges from 0 to 1, where 0 means the model explains none of the variability in the data, and 1 means the model explains all of the variability in the data.\n",
    "\n",
    "* Mean Squared Error (MSE): MSE measures the average squared difference between the actual and predicted values of the dependent variable. It is a measure of the model's accuracy.\n",
    "\n",
    "* Root Mean Squared Error (RMSE): RMSE is the square root of the MSE. It measures the average deviation of the predictions from the actual values of the dependent variable. \n",
    "  - Interpretability is a bit easier than MSE\n",
    "\n",
    "* Mean Absolute Error (MAE): MAE measures the average absolute difference between the actual and predicted values of the dependent variable. It is less sensitive to outliers than MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Function to Print Metics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def linear_model_metrics(y_test, y_pred):\n",
    "    print(\"Mean Absolute Error:    \", metrics.mean_absolute_error(y_test, y_pred))\n",
    "    print(\"Mean Squared Error:     \", metrics.mean_squared_error(y_test, y_pred))\n",
    "    print(\"Root Mean Squared Error:\", np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "    print(\"R-Squared Value:        \", metrics.explained_variance_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Use Fitted Model to Make Predictions\n",
    "\n",
    "### $$y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + ... \\beta_p x_{pi}$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Use our fitted model to make predictions using test set \n",
    "\n",
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print our metrics of model adequacy \n",
    "\n",
    "linear_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Minnesota Traffic Volume Data\n",
    "Hourly Interstate 94 Westbound traffic volume for MN DoT ATR station 301, roughly midway between Minneapolis and St Paul, MN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "traffic_df = pd.read_csv(\"data/Metro_Interstate_Traffic_Volume.csv\")\n",
    "\n",
    "print(traffic_df.shape)\n",
    "\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xvars = [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\"]\n",
    "\n",
    "X = traffic_df.loc[:, xvars].values              # get X values (i.e., predictors/features)\n",
    "y = traffic_df.loc[:, \"traffic_volume\"].values   # get y values (i.e., outcome/target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train/Test Split (the easy way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split training/test data at random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod = LinearRegression()       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)      # fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mod.coef_)               # show regression coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Making Predictions with Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use our fitted model to make predictions using test set \n",
    "\n",
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print our metrics of model adequacy \n",
    "\n",
    "linear_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "# Feature Engineering\n",
    "\n",
    "* The term \"feature\" is typically used to refer to predictor variables \n",
    "* Transforming and/or modifying data in a manner that extracts additional information from raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Pre-Processing and Data Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def get_hour_wkday(v_dt):\n",
    "    n = len(v_dt)\n",
    "    hour = np.zeros(n)\n",
    "    wkday = np.zeros(n)\n",
    "    \n",
    "    for i in range(n):\n",
    "        dt_tmp = time.strptime(v_dt[i], \"%Y-%m-%d %H:%M:%S\")\n",
    "        hour[i] = dt_tmp.tm_hour\n",
    "        wkday[i] = dt_tmp.tm_wday\n",
    "    \n",
    "    return hour, wkday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create two new columns `hour` and `wkday`\n",
    "\n",
    "traffic_df[\"hour\"], traffic_df[\"wkday\"] = get_hour_wkday(traffic_df[\"date_time\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Add dummy coded weather\n",
    "\n",
    "weather_dummy_codes = pd.get_dummies(traffic_df[\"weather_description\"])\n",
    "\n",
    "traffic_df = traffic_df.join(weather_dummy_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More Data Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create column of 0/1 indicating holidays\n",
    "\n",
    "traffic_df[\"is_holiday\"] = [0 if x == \"None\" else 1 for x in traffic_df[\"holiday\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create column of 0/1 indicating holidays\n",
    "\n",
    "traffic_df[\"is_weekend\"] = [1 if x in [5, 6] else 0 for x in traffic_df[\"wkday\"]]\n",
    "\n",
    "traffic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xvars = [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\"]\n",
    "\n",
    "xvars2 = [\"temp\", \"rain_1h\", \"snow_1h\", \"clouds_all\", \"hour\", \"wkday\", \"is_weekend\", \"is_holiday\"]\n",
    "\n",
    "xvars2 = xvars2 + weather_dummy_codes.columns.values.tolist()\n",
    "\n",
    "\n",
    "X = traffic_df.loc[:, xvars2].values             # get X values (i.e., predictors/features)\n",
    "y = traffic_df.loc[:, \"traffic_volume\"].values   # get y values (i.e., outcome/target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Split Training/Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split training/test data at random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod = LinearRegression()       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)      # fit model to training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mod.coef_)               # show regression coefficients "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use our fitted model to make predictions using test set \n",
    "\n",
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print our metrics of model adequacy \n",
    "\n",
    "linear_model_metrics(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Logistic Regression Models\n",
    "\n",
    "### Classification vs. Regression\n",
    "\n",
    "* Classification involves categorical outcome variable\n",
    "* Regression involves continuous outcome variable \n",
    "* \"logistic regression\" is used for modeling categorical data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=images/train_test_split.png width = 1080/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# `adult` Data\n",
    "  * Popular census data \n",
    "  * UCI ML repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "adult_df = pd.read_csv(\"data/adult.csv\", skipinitialspace = True)\n",
    "\n",
    "adult_df.columns = adult_df.columns.str.replace(\" \", \"\")\n",
    "\n",
    "print(adult_df.shape)\n",
    "\n",
    "adult_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Dummy Code Outcome Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "adult_df[\"income_gt_50\"] = [1 if x == \">50K\" else 0 for x in adult_df[\"income\"]]\n",
    "\n",
    "np.mean(adult_df[\"income_gt_50\"])        # proportion > 50k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xvars = [\"age\", \"education_num\", \"capital_gain\", \"hours_per_week\"]\n",
    "\n",
    "X = adult_df.loc[:, xvars].values              # get X values (i.e., predictors/features)\n",
    "y = adult_df.loc[:, \"income_gt_50\"].values     # get y values (i.e., outcome/target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Split training/test data at random\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod = LogisticRegression()       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)        # fit model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(mod.coef_)               # show regression coefficients "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Making Predictions with Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Use our fitted model to make predictions using test set \n",
    "\n",
    "y_pred = mod.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Print our metrics of model adequacy \n",
    "\n",
    "print(\"F1 Score:    \", metrics.f1_score(y_test, y_pred))\n",
    "print(\"Accuracy:    \", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is a good model?\n",
    "  * What is accuracy?\n",
    "  * What is our \"baseline\" accuracy?\n",
    "  * What other ways can we assess classification performance?\n",
    "      - Accuracy\n",
    "      - _F1_ score\n",
    "      - Sensitivity\n",
    "      - Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is accuracy?\n",
    "\n",
    "![image](images/true_false_positive.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Definitions\n",
    "\n",
    "* _True Positive_: Sick people correctly identified as sick\n",
    "* _False Positive_: Healthy people incorrectly identified as sick\n",
    "* _True Negative_: Healthy people correctly identified as healthy\n",
    "* _False Negative_: Sick people incorrectly identified as healthy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is Accuracy (cont.)?\n",
    "\n",
    "![image](images/true_false_positive_general.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is Accuracy (cont.)?\n",
    "\n",
    "<br>\n",
    "<center>$\\LARGE{Accuracy = \\frac{TP + TN}{TP + TN + FN + FP}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is \"baseline\" Accuracy?\n",
    "\n",
    "  * No diffinitive answer\n",
    "  * Basically, the accuracy we get with a \"simple model\"\n",
    "  * Predicting majority class for all "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is a \"good\" Model?\n",
    "\n",
    "  * Is 99% accuracy good?\n",
    "    - Credit card transaction example\n",
    "  * What are doing with our model? And what kind of errors bother us more?\n",
    "    - Diagnosing disease? (prioritize minimizing false negative)\n",
    "    - Filtering spam email? (prioritize minimizing false positives)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sensitivity and Specificity\n",
    "\n",
    "* Going beyond just accuracy \n",
    "* Decide what kind of error are worse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sensitivity\n",
    "\n",
    "* Also known as: \"hit rate\", \"true-positive rate\", \"recall\"\n",
    "* Answers this question:\n",
    "  - _\"What proportion of the positive cases are we correctly identifying?\"_\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "    \n",
    "<center>$\\LARGE{Sensitivity = \\frac{TP}{TP + FN} =  \\frac{TP}{P}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Specificity\n",
    "\n",
    "* Also known as: \"true-negative rate\"\n",
    "* Answers this question:\n",
    "  - _\"What proportion of the negative cases are we correctly identifying?\"_\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "    \n",
    "<center>$\\LARGE{Specificity = \\frac{TN}{TN + FP} =  \\frac{TN}{N}}$</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specificity (cont.)\n",
    "\n",
    "  * Also important because _false positive rate_ is $1 - Specificity = \\frac{FP}{FP + TN}$\n",
    "  * And minimizing false positives is often important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What is a \"good\" Model (cont.)?\n",
    "\n",
    "* It depends.\n",
    "* A good model is one that: \n",
    "  - is useful\n",
    "  - satisfies our requirements\n",
    "  - is wrong in the \"better\" way for our problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Question</h1></center>\n",
    "\n",
    "Let's write a function callend `sensitivity()` that computes the sensitivity of a classifier model's predicted output. Our function should take two arguments: `y` and `y_hat`, which are NumPy arrays. The function should then  return a value between `0` and `1` indicating the model's sensitivity.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center>$\\Large{Sensitivity = \\frac{TP}{TP + FN} =  \\frac{TP}{P}}$</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Write our function here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing our function\n",
    "import numpy as np\n",
    "\n",
    "y1 = np.array([1, 1, 0, 0, 1, 1]) # this is our ground truth (i.e., `y`)\n",
    "y2 = np.array([1, 0, 0, 1, 1, 1]) # this is our prediction (i.e., `y_hat`)\n",
    "\n",
    "sensitivity(y1, y2)               # should print `0.75`"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
