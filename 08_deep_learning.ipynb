{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "736d11df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Very Brief Introduction to Deep Learning</h1></center>\n",
    "<center><h3>Paul Stey</h3></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac1b43f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Impact of Deep Learning\n",
    "\n",
    "It would be difficult the overstate the impact.\n",
    "  * AI chatbots\n",
    "  * Facial recognition\n",
    "  * Image processing\n",
    "  * Voice recognition \n",
    "  * Medical imaging (e.g., tumor detection, pathology)\n",
    "  * Self-driving cars\n",
    "  * Game-playing AIs\n",
    "  * Virtual assistants (e.g., Siri, Alexa, Google)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea0fd5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Nomenclature\n",
    "\n",
    "Deep learning is a family modeling approaches with many names:\n",
    "\n",
    "  * Neural networks (NN)\n",
    "  * Deep neural networks (DNN)\n",
    "  * Artificial neural networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c477a986",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Network Basics\n",
    "\n",
    "\n",
    "What is a neural network?\n",
    "  * Universal function approximator\n",
    "  * A species of directed acyclic graphs (usually)\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19e3a36",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What do neural networks do?\n",
    "\n",
    "Like many other statistical or machine learning models (e.g., GLM, random forests, boosting), neural networks:\n",
    "  * Attempt to approximate a data-generating mechanism\n",
    "  * Can be used for classification problems\n",
    "  * Can be used for regression problems\n",
    "  * Can also be used for dimension reduction like principal components analysis (PCA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d31ef3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural Networks vs. other ML Modeling\n",
    "\n",
    "Similarities to other types of machine learning models\n",
    "\n",
    "  * Input variables (i.e., _**X**_, features, predictors, etc.) and output variable (i.e., _y_)\n",
    "\n",
    "  \n",
    "<center><img src=\"images/input_output.png\" width=420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ad3d08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Applications of Deep Learning\n",
    "Deep learning is extremely flexible, and can be applied to many domains.\n",
    "  \n",
    "<center><img src=\"images/self-driving_car.jpg\" width=860/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581bd77d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History of Neural Networks\n",
    "\n",
    "* Neural networks have been around since the 1940s, with the first artificial neuron being created by Warren McCulloch and Walter Pitts in 1943.\n",
    "* In the 1950s and 1960s, researchers developed the first neural network algorithms, including the Perceptron algorithm, which was capable of learning to classify images.\n",
    "* During the 1970s and 1980s, neural networks fell out of favor as researchers found it difficult to train them effectively. However, the development of backpropagation, a learning algorithm that could effectively train deep neural networks, in the 1980s paved the way for their resurgence.\n",
    "* In the 1990s, researchers developed Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs), which improved the ability of neural networks to handle complex data such as images and sequences.\n",
    "* In the 2010s, advances in computing power and the availability of large datasets led to a rapid increase in the use of deep learning for a wide range of applications, including computer vision, natural language processing, and speech recognition. \n",
    "* Today, deep learning is one of the most active areas of research in artificial intelligence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3ea08",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### More Recently\n",
    "\n",
    "Neural networks are experiencing a major resurgence. There are at least three reasons.\n",
    "\n",
    "  * Better algorithms for back-propagation\n",
    "  * GPUs are well suited to building neural networks\n",
    "    - Matrix multiplies can be made embarrassingly parallel \n",
    "    - GPUs have much better memory bandwidth\n",
    "  * More labeled data\n",
    "  \n",
    "  \n",
    "<center><img src=\"images/two_johns.jpg\" width=380/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7f0e35",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "An early and fairly straightforward example of a neural network.\n",
    "\n",
    "<center><img src=\"images/neural_network3.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47434d55",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Single Neuron\n",
    "\n",
    "A single neuron takes inputs, $x_j$, and applies the weights, $w_{\\cdot j}$ to the input by computing the dot product of the vectors $x$ and $w$. The result is the input to the \"activation function\".\n",
    "\n",
    "<center><img src=\"images/neuron2.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dc9a66",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "Larger networks can have many, _many_ weights!\n",
    "  * Origin of the term \"deep\" neural networks \n",
    "  * Largest models have _trillions_ of weights (i.e., parameters)\n",
    "\n",
    "<center><img src=\"images/neural_network4.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904fb8c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Activation Functions\n",
    "  * The notion of an activation function comes again from the conceptual relationship to neurons in the brain.\n",
    "\n",
    "  * Activation functions are analogous to \"link\" functions in generalized linear models (GLMs). \n",
    "    \n",
    "  * In fact, one common activation function is the sigmoid function, which is just our old friend the logistic function which you are using when you fit logistic regression models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbcfa29",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Purpose of Activation Functions\n",
    "\n",
    "There are a few reasons we use activation functions.    \n",
    "\n",
    "  * Need to take some linear predictor and transform it so that it is bounded appropriate. For instance, the value of logistic function is in the range $(0, 1)$. \n",
    "  * Allows us to introduce non-linearities. \n",
    "    - Approximate a data-generating mechanism \n",
    "    - Trying to approximate a function that might be very complicated and include non-linearities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54750a0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Activation Functions\n",
    "\n",
    "Some common activation functions include the following: \n",
    "  * Sigmoid (i.e., logistic)\n",
    "  * Hyperbolic tangent: $tanh$\n",
    "  * Rectified linear unit (ReLU)\n",
    "  * softplus\n",
    "  \n",
    "<center><img src=\"images/activation_functions.png\" width = 420/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405325a1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Question</h1></center>\n",
    "\n",
    "The sigmoid and the ReLU activation functions are two of the most common in deep learning. The formulas for these are below. Write a `sigmoid()` and a `relu()` function in Python that implements these.\n",
    "\n",
    "$$sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "$$relu(x) = \\text{max}(0, x)$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "**Hint:** Note that the NumPy module has the `e` constant included as a  part of the module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6da95",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "# Varieties of Neural Network (and layers)\n",
    "\n",
    "1. The \"feed-forward\" layer/network\n",
    "  * Mult-layer perceptron is a feed-foward network\n",
    "  * Most networks involve at least _some_ feed-foward layer\n",
    "2. Convolutional neural network (CNN)\n",
    "  * Ubiquitous in computer vision (i.e., image classification, object detection, facial recognition)\n",
    "3. Recurrent neural networks (RNN)\n",
    "  * Long short-term memory (LSTM) networks\n",
    "4. Generative adversarial network (GAN)\n",
    "  * Widely used in game-playing AI\n",
    "5. Autoencoders\n",
    "\n",
    "6. Transformers\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35cc85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks (CNNs)\n",
    "\n",
    "* Regular neural nets don't scale well to images\n",
    "  - For images of size $32 \\times 32 \\times 3$, a _single_ fully-connected neuron in the first layer would have $3072$ weights.\n",
    "  - Images of size $200 \\times 200 \\times 3$, a _single_ neuron gives $120000$ weights.\n",
    "* Full connectivity is wasteful and the huge number of parameters would quickly lead to overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039e94f6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## CNNs (cont.)\n",
    "What are CNNs?\n",
    "  * ConvNets are very similar to neural networks discussed thus far. Dot product, followed by non-linearity, and loss function at the end.\n",
    "  * Explicit assumption that input are images.\n",
    "  * Layers have neurons arranged in 3 dimensions (width, height, depth) to form an **activation volume**\n",
    "  \n",
    "<center><img src=\"images/cnn.png\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ccde69",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center><img src=\"images/convolution.gif\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1289b197",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Orginal Image\n",
    "<center><img src=\"images/building.jpg\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341cc2d9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apply Sobel operator filter\n",
    "\n",
    "<center><img src=\"images/building_sobel.jpg\" width=\"750\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6577e4d2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Architecture of CNN\n",
    "\t\n",
    "\n",
    "Types of layers used to build ConvNets\n",
    "  * Convolutional Layer\n",
    "    - Input: 3-d volume\n",
    "    - Output: 3-d volume\n",
    "    - Convolutional \"filters\" with small regions in the image\n",
    "    - Output depth, depends on the number of filters\n",
    "  * Pooling Layer\n",
    "    - Downsampling along spatial dimensions (width, height)\n",
    "  * Fully-Connected Layer (what we've seen so far)\n",
    "    - Compute class score. Dimensions are transformed to $1 \\times 1 \\times k$, where $k$ is number of classes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbefaec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Training vs. Inference\n",
    "\n",
    "1. Training neural network\n",
    "  * Process that computes weights (i.e., parameter estimates)\n",
    "  * Can take hours, days, weeks, or months\n",
    "  * Typically done on specialized hardware\n",
    "    - GPUs, TPUs, FPGAs\n",
    "2. Inference\n",
    "  * Use existing network (i.e., weights)\n",
    "  * Make predictions (i.e., classification, or numerical prediction)\n",
    "  * Happens fast; in many cases _extemely_ fast (e.g., milliseconds)\n",
    "  * Needs to happen on all kinds of devices (e.g., phones, cameras, sensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ed076d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Deep Learning Packages\n",
    "\n",
    "1. TensorFlow\n",
    "  * Free, open-source software\n",
    "  * Primarily developed by Google\n",
    "  * C++ library, callable from C++, Python, or R\n",
    "  * Created by Google\n",
    "2. Keras\n",
    "  * \"Front-end\" API for TensorFlow\n",
    "3. PyTorch\n",
    "  * Free, open-source software\n",
    "  * Developed in large part by Facebook\n",
    "  * C++/Cython library callable from Python\n",
    "  * Created by Facebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4bc00d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Hyperparameters \n",
    "\n",
    "One of the nuances of deep learning models is there are often many hyperparameters that can be tuned. These are aspects of the model that can have a huge impact on the model's accuracy, rate of convergence, and total time for training and inference. \n",
    "\n",
    "Here are some of the most consequential hyperparameters:\n",
    "\n",
    "* Learning rate\n",
    "\n",
    "* Batch size\n",
    "\n",
    "* Number of layers\n",
    "\n",
    "* Number of units per layer\n",
    "\n",
    "* Activation function\n",
    "\n",
    "* Dropout layers\n",
    "\n",
    "* Number of epochs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ca63c2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning Rate\n",
    "\n",
    "* Controls the step size taken by the optimization algorithm when updating the model's weights. \n",
    "\n",
    "* It determines the rate at which the model learns from the data and converges to the optimal solution.\n",
    "\n",
    "* A smaller learning rate corresponds to smaller weight updates, resulting in a slower convergence towards the optimal solution. \n",
    "  - While this can lead to more precise and stable convergence, it can also make the training process take longer and increase the risk of getting stuck in local minima.\n",
    "  \n",
    "* A larger learning rate leads to more significant weight updates, which can speed up the training process and allow the model to escape local minima. \n",
    "  - However, using a learning rate that is too large may cause the model to overshoot the optimal solution, resulting in oscillation or divergence, and poor convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e15ed35",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Batch Size \n",
    "\n",
    "* The batch size is an important hyperparameter in deep learning that controls the number of training examples used in a single update of the model's weights during training. \n",
    "  - It is the number of samples that are processed simultaneously in each iteration of the training process.\n",
    "  \n",
    "* Smaller batch sizes result in noisier gradient estimates, which can help escape local minima and promote better generalization. \n",
    "  - However, this may also lead to slower convergence. Larger batch sizes provide more accurate gradient estimates, which can lead to faster convergence but may risk getting stuck in local minima.\n",
    "  \n",
    "* Larger batch sizes require more memory to store intermediate values, both in terms of GPU/TPU memory and system memory (RAM). \n",
    "  - This can become a limiting factor, especially when working with large models and high-resolution data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0310ea3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of layers\n",
    "\n",
    "* Defines the depth of a neural network, which influences the model's capacity to learn complex patterns and representations. \n",
    "\n",
    "* Deeper networks can potentially learn more intricate features, but they are also more prone to overfitting and require more computational resources.\n",
    "\n",
    "* More shallow networks require less resources and time for training, but can underfit the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090dde8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of Units Per Layer\n",
    "\n",
    "* Determines the width of each layer in a neural network. More units per layer can increase the expressive power of the model, but may also lead to overfitting and increased computational complexity.\n",
    "\n",
    "* More units per layer also increases computational burden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39db69ee",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Activation Function\n",
    "\n",
    "* The non-linear function applied to the output of each layer, which introduces non-linearity into the model and allows it to learn complex mappings. \n",
    "\n",
    "* Common activation functions include ReLU, sigmoid, and tanh."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8004759",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Number of Epochs\n",
    "\n",
    "* The number of epochs in training a deep learning model refers to the number of times the entire training dataset is passed through the model during the training process. \n",
    "\n",
    "* An epoch consists of multiple iterations, where each iteration processes a batch of training examples and updates the model's weights based on the computed gradients.\n",
    "\n",
    "* _Underfitting_: If the number of epochs is too low, the model may not have enough time to learn the underlying patterns in the data, leading to underfitting. In this case, the model performs poorly on both the training and validation datasets.\n",
    "\n",
    "* _Overfitting_: If the number of epochs is too high, the model can start to memorize the training data, leading to overfitting. In this case, the model performs well on the training dataset but poorly on unseen data or the validation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727bfd8b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Dropout Layers\n",
    "\n",
    "* _Regularization technique_: Used to prevent overfitting by randomly dropping out, or \"deactivating,\" a proportion of neurons in a layer during training.\n",
    "\n",
    "* _Dropout rate_: Determines the fraction of neurons to be deactivated in a given layer; typically set to a value between 0 and 1 (e.g., 0.5 corresponds to dropping out 50% of the neurons)\n",
    "\n",
    "* _Training-time only_: Dropout is applied only during training. When the model is used for inference or evaluation, all neurons are active, and their outputs are scaled down by the dropout rate to compensate for the increased number of active neurons compared to training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200c6e5c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef756572",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X, y = fetch_openml('mnist_784', version = 1, return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f480021",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def show_image(x):\n",
    "    x_resize = np.array(x).reshape(28, 28)\n",
    "    plt.imshow(x_resize, \n",
    "               cmap = \"Blues\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0910a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 137                    # image number\n",
    "show_image(X.iloc[n])      # plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a34fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X.values, \n",
    "                                                    y.values.astype(int), \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb4229",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch               # this may take a bit of time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691c6a97",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7c53d5",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Feed forward neural network definition\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15188238",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 524            # number of neurons in hidden layers\n",
    "batch_size = 100             # number of samples in each iteration\n",
    "num_epochs = 2               # number of full passes through training set\n",
    "learning_rate = 0.1          # step size for optimization algorithm\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = data.TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Net(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7521e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "itr = 1\n",
    "print_itr = 200\n",
    "total_loss = 0\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if itr % print_itr == 0:\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            acc = torch.mean(correct.float())\n",
    "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, num_epochs, itr, total_loss/print_itr, acc))\n",
    "            loss_list.append(total_loss/print_itr)\n",
    "            acc_list.append(acc)\n",
    "            total_loss = 0\n",
    "        \n",
    "        itr += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc650da7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_accuracy(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)                        # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nAverage loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535c2c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = data.TensorDataset(torch.Tensor(X_test), torch.LongTensor(y_test))\n",
    "dataloader_test = data.DataLoader(dataset_test, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b9c984",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f06d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_accuracy(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe463d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Neural network definition\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net2, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.20)\n",
    "        self.fc3 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.40)\n",
    "        self.fc4 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout3 = nn.Dropout(0.60)\n",
    "        self.fc5 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.fc5(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbe061a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "num_samples = X_train.shape[0]\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 424           # number of neurons in hidden layers\n",
    "batch_size = 250              # number of samples in each iteration\n",
    "num_epochs = 3              # number of full passes through training set\n",
    "learning_rate = 0.5       # step size for optimization algorithm\n",
    "\n",
    "\n",
    "# Create DataLoader\n",
    "dataset = data.TensorDataset(torch.Tensor(X_train), torch.LongTensor(y_train))\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = Net2(input_size, hidden_size, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64e0b97",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "itr = 1\n",
    "print_itr = 200\n",
    "total_loss = 0\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for inputs, labels in dataloader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        if itr % print_itr == 0:\n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            correct = pred.eq(labels)\n",
    "            acc = torch.mean(correct.float())\n",
    "            print('[Epoch {}/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, num_epochs, itr, total_loss/print_itr, acc))\n",
    "            loss_list.append(total_loss/print_itr)\n",
    "            acc_list.append(acc)\n",
    "            total_loss = 0\n",
    "        \n",
    "        itr += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f9b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model, dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb1ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy(model, dataloader)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
