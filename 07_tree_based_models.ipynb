{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Tree-Based Models</h1></center>\n",
    "<center><h3>2023-03-15</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are \"Tree-based Methods\"?\n",
    "\n",
    "  * Species of supervised machine learning models\n",
    "  * These models are more \"algorithmic\", not so formulaic\n",
    "  * Basic idea is to build \"decision trees\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"images/amiahorse.jpg\" width=480/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Species of Tree-Based Methods\n",
    "\n",
    "  * ID3 (Quinlan, 1986)\n",
    "  * C4.5 (Quinlan, 1993)\n",
    "  * C5.0 (Quinlan, 1996)\n",
    "  * CART (Breiman, Friedman, Olshen, & Stone, 1984)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classification and Regression Trees\n",
    "\n",
    "* A decision tree can be used for both classification and regression tasks\n",
    "* Tree's internal node represents a test on an attribute or feature, each branch represents the outcome of the test, and each leaf node represents a class label or a target value.\n",
    "* Tree-building algorithms recursively partition the feature space into smaller subspaces based on the values of the features, using the most informative feature at each step to create the split\n",
    "  - The process continues until a stopping criterion is met, such as reaching a certain depth or having a minimum number of samples in each leaf node.\n",
    "* Advantages of decision trees:\n",
    "  - Easy to understand and interpret\n",
    "  - Can handle both categorical and continuous input features\n",
    "  - Can capture complex relationships between the features and the target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### CART Algorithm Pseudo-code\n",
    "![image](images/cart_pseudo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Titanic Example\n",
    "\n",
    "  * Titantic survival data\n",
    "  * Kaggle competition\n",
    "  * Predicting survival (2-class problem)\n",
    "  * Using demographic and other variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "titanic_df = pd.read_csv(\"data/titanic_subset.csv\")\n",
    "\n",
    "titanic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Titanic CART Tree\n",
    "<img src=images/tree.png height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Advantages of CART Trees\n",
    "  * Intuitive\n",
    "  * Viable when $n \\ll p$\n",
    "  * Handle interactions naturally\n",
    "  * Minimal assumptions\n",
    "  * Handle missingness in predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Disadvantages of Single Trees\n",
    "  * High variance\n",
    "  * Prone to overfitting\n",
    "  * Lack of smoothness (troubling for regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bootstrap (Quick Detour)\n",
    "* The bootstrap is a resampling technique used to estimate the sampling distribution of a statistic or to quantify the uncertainty associated with a point estimate\n",
    "* Involves drawing a large number of random samples with replacement from the original dataset to create \"bootstrap samples\"\n",
    "* For each bootstrap sample, the statistic of interest (such as the mean or median) is computed, and the distribution of these statistics across all bootstrap samples is used to estimate the sampling distribution of the statistic.\n",
    "* Can be used to compute confidence intervals and p-values without making distributional assumptions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bootstrap Aggregation (\"Bagging\")\n",
    "  * Breiman (1996)\n",
    "  * Extends idea of CART models\n",
    "    - Single trees overfit\n",
    "    - Stopping rules and pruning help, but only to a point\n",
    "  * Take $B$ bootstrap samples, build $B$ trees, aggregate predictions\n",
    "    - For regression, aggregation is taking the mean of the predicted values for each $y_i$\n",
    "\t- For classification, to aggregate means each tree casts a ``vote'' for each $y_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Bagging Pseudo-Code\n",
    "![image](images/bagging_pseudo_code.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Advantages of Bagging\n",
    "  * Big improvement in predictive performance\n",
    "  * Variance decreases\n",
    "  * Easy to tune\n",
    "  * Embarrassingly parallel\n",
    "  * Out-of-bag (OOB) error estimate (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Problem</h1><center>\n",
    "    \n",
    "Let's write our own function for returning bootstrap samples. Let's call our function `bootstrap()`. It should take two arguments, `data` and `n_boot`; the first is a 1-dimmensional array, and the second is an integer. Our function should return a 2-dimmensional NumPy array with `n` rows and `n_boot` columns, where `n` is the length of `data` array, and where each column is a bootstrap sample from the origin `data`. \n",
    "    \n",
    "There are a few ways that we could do this, but we will almost certainly want to use the `np.random.choice()` function in NumPy to sample from a 1-D array.\n",
    "    \n",
    "**HINT**: Recall also that we can use the `np.zeros()` function to allocate an array of arbirary dimensions that's filled with zeros. This is frequently useful when we want to \"pre-allocated\" an array that we later fill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Define our function here \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Testing our fuction\n",
    "x = np.random.normal(0, 1, 1_000_000)\n",
    "\n",
    "boot_samples = bootstrap(x, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Should print `True` 3 times\n",
    "print(boot_samples.shape == (1_000_000, 100))\n",
    "print(-0.1 < np.mean(boot_samples) < 0.1)\n",
    "print(0.9 < np.std(boot_samples) < 1.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Random Forests\n",
    "  * Ho (1995), Breiman (2001)\n",
    "  * Extends idea of bagging\n",
    "  * Build many trees, aggregate predictions\n",
    "  * Only consider $m$ predictors at each node\n",
    "      - These $m$ predictors are selected at random\n",
    "  * Improve predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests Pseudo-Code\n",
    "![image](images/random_forests_pseudocode.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Out-of-Bag (OOB) Samples\n",
    "  * For each bootstrap iteration, we have some $(y_i, \\bf{x_i})$  that weren't used in tree building\n",
    "  * Use these $(y_i, \\bf{x}_i)$ OOB samples to estimate test error\n",
    "  * Also use these $(y_i, \\bf{x_i})$ to generate measures of variable importance (more on this later)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## OOB and Test Error\n",
    "![image](images/oob_error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Random Forests and Overfitting\n",
    "\n",
    "  * It was once (mistakenly) believed that random forests would not over fit\n",
    "  * You _can_ overfit by growing deep trees\n",
    "  * Growing too many trees won't cause you to overfit (but it's wasteful of computing resources)\n",
    "  * In general, people worry _much_ less about overfitting in random forests (relative to other approaches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Fitting Random Forest\n",
    "\n",
    "Fitting random forests models using Python is fairly straightforward. There is an excellent implementation of random forests in the scikit-learn package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn import metrics\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MNIST Data\n",
    "* Handwritten digits\n",
    "* Ten-class classification problem (i.e., 0-9 classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version = 1, return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# quick function to plot image from MNIST\n",
    "\n",
    "def show_image(x):\n",
    "    x_resize = np.array(x).reshape(28, 28)\n",
    "    plt.imshow(x_resize, \n",
    "               cmap = \"Blues\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "n = 137                    # image number\n",
    "show_image(X.iloc[n])      # plot image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(y[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Split Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size = 0.2, \n",
    "                                                    random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting our Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create model object\n",
    "\n",
    "rfc = RandomForestClassifier(n_jobs = 2, \n",
    "                             n_estimators = 100,\n",
    "                             oob_score = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.fit(X_train, y_train)     # fit our model (takes time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.oob_score_               # Out-of-bag error estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rfc.score(X_test, y_test)    # test error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Contrast with Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mod = LogisticRegression(max_iter = 10000, solver = \"saga\")       # create model object\n",
    "\n",
    "mod.fit(X_train, y_train)               # fit model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = mod.predict(X_test)            # use fitted model to make predictions\n",
    "\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Boosted Trees\n",
    "\n",
    "* Boosted trees and random forests are both ensemble learning methods that combine multiple decision trees\n",
    "* But boosted trees and random forests differ in several key aspects\n",
    "  - _Tree-building strategy_: boosted trees are built sequentially\n",
    "  - _Cominbation strategy_: boosted trees are combined using a weighted sum, where weights depend on performance of each tree in reducing the loss function. Random forests essentially do majority vote (classification) or average of predictions (regression)\n",
    "  - _Overfitting_: Boosted trees are more prone to overfitting, especially when the number of trees is large or the learning rate is high. Random forests are less prone to overfitting, as the random feature selection at each split introduces diversity and reduces correlation among trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient Boosted Tree Algorithms\n",
    "\n",
    "* AdaBoost (Adaptive Boosting):\n",
    "  - One of the earliest boosting algorithms\n",
    "  - Uses a simple reweighting of the samples based on the errors made by the previous trees\n",
    "  - Generally limited to binary classification problems\n",
    "\n",
    "* Gradient Boosting Machines (GBMs):\n",
    "  - Builds trees based on the gradient of the loss function, making it more general and applicable to various loss functions and problem types\n",
    "  - Used for both classification and regression tasks\n",
    "  - Introduces a learning rate parameter to control model complexity\n",
    "\n",
    "* XGBoost (eXtreme Gradient Boosting):\n",
    "  - An optimized and scalable version of GBMs\n",
    "  - Offers additional regularization techniques to control overfitting, such as L1 and L2 regularization\n",
    "  - Supports parallel and distributed computing, making it faster and more efficient\n",
    "  - Can handle missing data and provides feature importance scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Fitting Boosted Tree Model in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "btc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "\n",
    "btc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
