{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e74d2525",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<center><h1>Introduction to Natural Language Processing (NLP)</h1></center>\n",
    "\n",
    "<center><h3>Paul Stey</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d97f3a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# What is natural language processing (NLP)?\n",
    "\n",
    "* NLP exists at the intersection of disciplines\n",
    "  + Linguistics\n",
    "  + Statistics\n",
    "  + Machine learning and artificial intelligence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae06f45d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Where is NLP Used?\n",
    "\n",
    "* Advertising and Marketing\n",
    "  + Understanding consumer preferences\n",
    "* Law\n",
    "  + Automated reading of discovery docs\n",
    "* Automated Journalism\n",
    "  + Machine-generated articles\n",
    "* Finance\n",
    "  + Quant funds \n",
    "  + \"Anne Hathaway\" problem\n",
    "* Medicine\n",
    "  + Automated analysis of clinical notes\n",
    "* Machine translation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4086608c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples of NLP Usage\n",
    "\n",
    "* Voice assistants (e.g., Siri, Alexa, etc.)\n",
    "* Google Translate\n",
    "* Auto-complete\n",
    "* Chatbots (e.g., ChatGPT, Bard, etc.)\n",
    "* Speech-to-text on phones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871f567",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## History of NLP\n",
    "\n",
    "* Formally studied in linguistics departments\n",
    "* From as early as 1950s\n",
    "* Early emphasis on rule-based methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9046ae",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Some Building Blocks\n",
    "\n",
    "* Tokenization\n",
    "  + A token is a string with known meaning\n",
    "* Stemming\n",
    "  + Chop off the ends of words\n",
    "  + Many different kinds of stemming\n",
    "  + `\"cooking\"` => `\"cook\"`\n",
    "  + `\"distribution\"` => `\"distribut\"`\n",
    "  \n",
    "* Lemmatization\n",
    "  + More sophisticated than stemming\n",
    "  + Uses vocabulary and context\n",
    "  + `\"am\"`, `\"are\"`, `\"is\"` could be mapped to `\"be\"` using lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1baca8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# spaCy Package in Python\n",
    "\n",
    "* Create by Matt Honnibal and Ines Montani \n",
    "* Amazingly powerful\n",
    "* Support for dozens of languages\n",
    "* Extremely fast!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512452df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag-of-words Model\n",
    "\n",
    "* Species of vector space model\n",
    "* Produce count vectors\n",
    "* Embedding words in a vector space\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9958c56",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install spacy\n",
    "!python3 -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a59db52",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "df = pd.read_csv(\"data/short_movie_reviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6350cb03",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef70152",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(df.iloc[0,1])\n",
    "\n",
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5d5da2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(df.iloc[3,1])\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fee7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h1>Challenge Problem</h1></center>\n",
    "\n",
    "In NLP stop words are common words that are often considered to be of little value in text analysis because they don't carry much meaningful information by themselves. Examples of stop words include articles (a, an, the), prepositions (in, on, at), conjunctions (and, or, but), and pronouns (he, she, it).\n",
    "\n",
    "The scpaCy package in Python has a built-in list of stop words, which is in this object: `nlp.Defaults.stop_words` \n",
    "\n",
    "Let's write a function called `remove_stopwords()` that takes a string of text as input, and returns the string with all the stop words removed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d8f34a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Parts-of-Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e3aec",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(df.iloc[3,1])\n",
    "\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc3edc0",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_tokens(col):\n",
    "    words = ' '.join(col.tolist()).lower()\n",
    "    tokens = set([str(word) for word in nlp(words) \n",
    "                  if word.pos_ != \"PUNCT\" \n",
    "                  and not word.is_stop])\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb94ad76",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "all_tokens = get_unique_tokens(df.review)\n",
    "\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f624aeec",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def word_counts_dataframe(df_raw, token_set):\n",
    "    df_new = df_raw.copy()\n",
    "    \n",
    "    n = df_new.shape[0]\n",
    "    for token in token_set:\n",
    "        df_new[token] = np.zeros(n, int)\n",
    "        \n",
    "        for (i, review) in enumerate(df_new.review):\n",
    "            df_new.loc[i, token] = df_new.loc[i, \"review\"].lower().count(token)\n",
    "    \n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fb0d4e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df_bow = word_counts_dataframe(df, all_tokens)\n",
    "\n",
    "df_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf73895d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## TF-IDF\n",
    "\n",
    "* Term frequency, Inverse document frequency\n",
    "  - Term frequency: how often is the term in this document\n",
    "  - Inverse document frequency: how rare is the term across collection of documents (i.e., corpus)\n",
    "  \n",
    "* Statistic that normalizes for relative \"importance\" of words\n",
    "\n",
    "$${\\displaystyle \\mathrm {tfidf} (t,d,D)=\\mathrm {tf} (t,d)\\cdot \\mathrm {idf} (t,D)}$$\n",
    "\n",
    "* \"Important\" term \n",
    "  + appears frequently in the document\n",
    "  + and is rare across all documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6567ce9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ddb51d",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "## implement tf/idf \n",
    "def tf(term, doc):\n",
    "    num_term = doc.lower().count(term)\n",
    "    res = num_term/len(doc.lower().split())\n",
    "    return res\n",
    "\n",
    "\n",
    "def idf(term, documents):\n",
    "    n = len(documents)\n",
    "    num_term = 0\n",
    "    \n",
    "    for doc in documents:\n",
    "        if term in doc.lower():\n",
    "            num_term += 1\n",
    "    \n",
    "    return np.log(n/num_term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9ce940",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### TF-IDF (cont.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df71031",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf(term, doc, documents):\n",
    "    tf_val = tf(term, doc)\n",
    "    idf_val = idf(term, documents)\n",
    "    \n",
    "    return tf_val * idf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2f181",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "idf(\"terrible\", df.loc[:, \"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d347754",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tf(\"terrible\", df.loc[11, \"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9ccd46",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tfidf(\"terrible\", df.loc[11, \"review\"], df.loc[:, \"review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1760a7de",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def tfidf_dataframe(df_raw, token_set):\n",
    "    df_new = df_raw.copy()\n",
    "    \n",
    "    n = df_new.shape[0]\n",
    "    for token in token_set:\n",
    "        df_new[token] = np.zeros(n, int)\n",
    "        \n",
    "        for (i, review) in enumerate(df_new.review):\n",
    "            df_new.loc[i, token] = tfidf(token, df_new.loc[i, \"review\"].lower(), df_new.loc[:, \"review\"])\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "df_tfidf = tfidf_dataframe(df, all_tokens)\n",
    "\n",
    "df_tfidf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9a947f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bag-of-Words, Vector Models, and Embeddings\n",
    "\n",
    "* Are all embeddings\n",
    "* All representations of text as vectors\n",
    "* Several famous word embedding models\n",
    "  + word2vec\n",
    "  + GLoVe\n",
    "* _Unbelievably powerful_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6894c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Token Similarity\n",
    "\n",
    "* Using embeddings, we can compute the similarity of documents, sentences, or tokens\n",
    "* similarity can be computed using distance metric (e.g., cosine similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40487f17",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t1 = nlp(\"lion\")\n",
    "t2 = nlp(\"tiger\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6b6168",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "t1.similarity(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b47d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc40058",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Sentence Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cb7bb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s1 = nlp(\"I went to the store today\")\n",
    "s2 = nlp(\"I will go to the market tomorrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a1872e",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "s1.similarity(s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4698538c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
